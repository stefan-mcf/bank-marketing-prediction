{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b0aa71d0",
   "metadata": {
    "id": "polished-intro"
   },
   "source": [
    "# Bank Marketing: Term Deposit Classifier\n",
    "\n",
    "This notebook presents an applied machine learning project to predict whether a client will subscribe to a term deposit using the UCI Bank Marketing dataset. It is framed as a binary classification task and demonstrates a clear, reproducible workflow that a data science team could hand off or extend.\n",
    "\n",
    "- **Dataset:** UCI Bank Marketing (`bank-full.csv`) with demographic and campaign features  \n",
    "- **Goal:** Build and evaluate models that estimate subscription probability for targeted outreach  \n",
    "- **Methods:** Preprocessing pipelines (`ColumnTransformer` + `StandardScaler`), Logistic Regression (L2, L1, Elastic Net), stratified cross-validation  \n",
    "- **Metrics:** Accuracy, ROC AUC, Precision, Recall, Confusion Matrix — with emphasis on class imbalance and thresholding  \n",
    "\n",
    "**Why it matters:** In banking and fintech, optimising contact strategy reduces cost and customer fatigue. Calibrated probabilities and threshold selection support decisions such as who to call, when to call, and how to allocate budget across segments. This aligns with my interests in applied ML and decision support systems."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaee7302",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "md-q1-load",
   "metadata": {},
   "source": [
    "### Load and Explore Data\n",
    "\n",
    "Load and explore the dataset to check its shape, feature names, and the class distribution of target variable `y`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19a031e5-b611-4999-aea4-76a58c2832b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('data/bank-full.csv', sep=';')\n",
    "\n",
    "# Shape of the dataset\n",
    "print('Shape:', df.shape)\n",
    "\n",
    "# First few rows\n",
    "df.head()\n",
    "\n",
    "# Feature names (columns)\n",
    "print('Features:', df.columns.tolist())\n",
    "\n",
    "# Class balance for target 'y'\n",
    "print('y value counts:')\n",
    "print(df['y'].value_counts())\n",
    "print('y proportions (%):')\n",
    "print((df['y'].value_counts(normalize=True) * 100).round(2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "md-q1-map-y",
   "metadata": {},
   "source": [
    "## Baseline Logistic Regression\n",
    "\n",
    "Build a baseline logistic regression model on the `bank-full.csv` dataset to predict whether a client subscribes to a term deposit.\n",
    "This involves:\n",
    "- Encoding categorical variables and scaling numeric features\n",
    "- Training a logistic regression with default parameters\n",
    "- Evaluating performance using accuracy, confusion matrix, ROC AUC, and precision/recall\n",
    "\n",
    "### Encode Target y\n",
    "\n",
    "Convert the target `y` into binary format (yes → 1, no → 0) and confirm counts/proportions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3d5f9b2-6e2a-4d3b-9b01-6a3c7e2d1c9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert target 'y' to binary (yes→1, no→0) and confirm distribution\n",
    "df['y'] = df['y'].map({'yes': 1, 'no': 0}).astype('int64')\n",
    "\n",
    "print('y counts (0/1):')\n",
    "print(df['y'].value_counts().sort_index())\n",
    "print('y proportions (%):')\n",
    "print((df['y'].value_counts(normalize=True) * 100).round(2).sort_index())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "md-q1-onehot",
   "metadata": {},
   "source": [
    "### Encode Features (One‑Hot)\n",
    "\n",
    "Encode the specified categorical features using one‑hot encoding (drop_first=True) and preview the transformed DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b3a2f40-c9e0-4c9f-9d85-6a2c1c5a0f31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-hot encode selected categorical features (drop_first to avoid redundancy)\n",
    "categorical_cols = ['job', 'marital', 'education', 'default', 'housing', 'loan', 'contact', 'month', 'poutcome']\n",
    "# Leave numeric columns as-is\n",
    "numeric_cols = ['age', 'balance', 'day', 'duration', 'campaign', 'pdays', 'previous']\n",
    "\n",
    "df = pd.get_dummies(df, columns=categorical_cols, drop_first=True, dtype='int64')\n",
    "\n",
    "# Preview the updated DataFrame\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "md-q1-split",
   "metadata": {},
   "source": [
    "### Stratified Train/Test Split\n",
    "\n",
    "Split the dataset into training and test sets (80/20) using stratified sampling to preserve the ≈88/12 class balance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a7e9c9c-9c0a-4a7b-8fcd-1f7d2a8b9e2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train/test split (80/20) with stratification to preserve class ratio\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = df.drop(columns=['y'])\n",
    "y = df['y']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.20, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print('X_train shape:', X_train.shape)\n",
    "print('X_test shape:', X_test.shape)\n",
    "print('y_train shape:', y_train.shape)\n",
    "print('y_test shape:', y_test.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "md-q1-pipeline",
   "metadata": {},
   "source": [
    "### Baseline Pipeline and Evaluation\n",
    "\n",
    "Standardise numeric features using StandardScaler within a pipeline to ensure stable optimisation, then evaluate with accuracy, confusion matrix, precision/recall/F1, and ROC AUC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c4b0d52-9d0a-4e9f-86a5-0b8f9f2ec7a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale numeric columns only, keep one-hot columns as-is; then train Logistic Regression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, roc_auc_score, classification_report\n",
    "\n",
    "numeric_cols = ['age', 'balance', 'day', 'duration', 'campaign', 'pdays', 'previous']\n",
    "\n",
    "col_tx = ColumnTransformer(\n",
    "    transformers=[('num', StandardScaler(), numeric_cols)],\n",
    "    remainder='passthrough'\n",
    ")\n",
    "\n",
    "pipe = Pipeline([\n",
    "    ('prep', col_tx),\n",
    "    ('clf', LogisticRegression(max_iter=1000, n_jobs=-1))\n",
    "])\n",
    "\n",
    "pipe.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate on test set\n",
    "y_pred = pipe.predict(X_test)\n",
    "y_proba = pipe.predict_proba(X_test)[:, 1]\n",
    "\n",
    "print('Accuracy:', round(accuracy_score(y_test, y_pred), 4))\n",
    "print('Confusion matrix:', confusion_matrix(y_test, y_pred))\n",
    "print('ROC AUC:', round(roc_auc_score(y_test, y_proba), 4))\n",
    "print('Classification report:', classification_report(y_test, y_pred, digits=3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "md-q1-wrap",
   "metadata": {},
   "source": [
    "### Summary\n",
    "\n",
    "The baseline logistic regression achieves strong ROC AUC (≈0.90) and overall accuracy (≈90%), but recall for the minority class is lower than precision. This shows accuracy alone can be misleading on imbalanced data and motivates cross‑validated hyperparameter tuning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05a32d87",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6e36cb9-6c7c-4a3a-9b3a-1b0b8e3f4d21",
   "metadata": {},
   "source": [
    "## Hyperparameter Tuning\n",
    "\n",
    "**Objective:**  \n",
    "Optimise the logistic regression model by tuning the regularisation strength (C) using stratified 5-fold cross-validation with ROC AUC scoring.  \n",
    "This involves:  \n",
    "- Defining a parameter grid for C\n",
    "- Running GridSearchCV to find the best parameter\n",
    "- Plotting ROC AUC vs C to visualise the effect of regularisation\n",
    "- Evaluating the tuned model on the test set and comparing results with \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "md-q2-pipeline",
   "metadata": {},
   "source": [
    "### Rebuild Pipeline for CV\n",
    "\n",
    "Recreate ColumnTransformer (scale numeric; passthrough one‑hots) and LogisticRegression (lbfgs, L2), so CV evaluates the same preprocessing+model pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7f3c7a1-2e3d-4c8c-9a2b-5d1f2e3c4b5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reuse pipeline defined components\n",
    "# - StandardScaler on numeric columns via ColumnTransformer (remainder='passthrough')\n",
    "# - LogisticRegression(solver='lbfgs', penalty='l2', max_iter=1000)\n",
    "prep = ColumnTransformer([('num', StandardScaler(), numeric_cols)], remainder='passthrough')\n",
    "pipe = Pipeline([('prep', prep), ('clf', LogisticRegression(solver='lbfgs', penalty='l2', max_iter=1000))])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "md-q2-define",
   "metadata": {},
   "source": [
    "### Define Hyperparameter Grid and CV\n",
    "\n",
    "Tune `C` (inverse regularisation) to control model complexity—smaller C = stronger regularisation (ties to SRM from Week 5). Use 5‑fold stratified CV to keep the 88/12 class ratio in each fold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "code-q2-define",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define hyperparameter grid and stratified CV strategy\n",
    "from sklearn.model_selection import StratifiedKFold, GridSearchCV\n",
    "\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "param_grid = {\n",
    "    'clf__C': [0.001, 0.003, 0.01, 0.03, 0.1, 0.3, 1, 3, 10, 30, 100]\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "md-q2-run",
   "metadata": {},
   "source": [
    "### Run GridSearchCV\n",
    "\n",
    "Perform stratified 5‑fold CV with ROC AUC as the selection metric (appropriate for imbalanced data). Report the best C and mean CV AUC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2c4d5f6-7a8b-49c0-9d1e-2f3a4b5c6d7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run GridSearchCV using the defined CV strategy and grid\n",
    "grid = GridSearchCV(\n",
    "    estimator=pipe,\n",
    "    param_grid=param_grid,\n",
    "    scoring='roc_auc',\n",
    "    cv=skf,\n",
    "    n_jobs=-1,\n",
    "    refit=True,\n",
    "    verbose=1,\n",
    "    return_train_score=True\n",
    ")\n",
    "\n",
    "grid.fit(X_train, y_train)\n",
    "\n",
    "print('Best params:', grid.best_params_)\n",
    "print('Best CV ROC AUC:', round(grid.best_score_, 4))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "md-q2-plot",
   "metadata": {},
   "source": [
    "### Plot ROC AUC vs C\n",
    "\n",
    "Visualise how performance varies with `C`. The curve typically rises from underfitting at very small C, peaks, then flattens, indicating diminishing returns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7d4a2e1-8f9b-4d3e-a6b2-1c3d5e7f9a0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot CV ROC AUC vs C (with error bars)\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "cvres = pd.DataFrame(grid.cv_results_)\n",
    "cvres['C'] = cvres['param_clf__C'].astype(float)\n",
    "cvres = cvres.sort_values('C')\n",
    "\n",
    "plt.figure(figsize=(7,4))\n",
    "plt.errorbar(cvres['C'], cvres['mean_test_score'], yerr=cvres['std_test_score'], fmt='-o', capsize=4)\n",
    "plt.xscale('log')\n",
    "plt.xlabel('C (inverse regularization)')\n",
    "plt.ylabel('Mean CV ROC AUC')\n",
    "plt.title('Logistic Regression: ROC AUC vs C (5-fold Stratified CV)')\n",
    "best_C = grid.best_params_['clf__C']\n",
    "plt.axvline(best_C, color='red', linestyle='--', alpha=0.6, label=f'Best C = {best_C}')\n",
    "plt.legend()\n",
    "plt.grid(True, which='both', ls=':', alpha=0.5)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4363a372",
   "metadata": {},
   "source": [
    "The ROC AUC vs C plot shows rapid gains moving away from very small C (strong regularisation, underfitting). The curve peaks around C ≈ 0.03 and then flattens, indicating diminishing returns from further increasing model complexity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "md-q2-test-eval",
   "metadata": {},
   "source": [
    "### Evaluate Tuned Model on Test\n",
    "\n",
    "Use the refit `best_estimator_` to score the held‑out test set and compare results with the baseline baseline (accuracy, confusion matrix, ROC AUC, precision/recall)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2e1c3b4-5f67-4a89-9cde-7ab8c9d0ef12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate tuned model on the held-out test set\n",
    "best_model = grid.best_estimator_\n",
    "y_pred = best_model.predict(X_test)\n",
    "y_proba = best_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "print('Test Accuracy:', round(accuracy_score(y_test, y_pred), 4))\n",
    "print('Test Confusion matrix:', confusion_matrix(y_test, y_pred))\n",
    "print('Test ROC AUC:', round(roc_auc_score(y_test, y_proba), 4))\n",
    "print('Test Classification report:', classification_report(y_test, y_pred, digits=3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "md-q2-summary",
   "metadata": {},
   "source": [
    "### Summary\n",
    "\n",
    "Best parameter: C = 0.03. Best CV ROC AUC = 0.9073. On the test set, performance is around Accuracy ≈ 0.90, ROC AUC ≈ 0.9069, Precision (positive) ≈ 0.65, Recall (positive) ≈ 0.32. Compared to the baseline, ROC AUC improves slightly (≈0.905 → ≈0.907); precision stays similar while recall dips a bit—consistent with a slightly stronger regularisation/thresholding trade‑off."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4896074",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "q3-writeup",
   "metadata": {},
   "source": [
    "## Regularisation Variants\n",
    "\n",
    "**Objective:**  \n",
    "Explore sparse (L1) and Elastic Net regularisation for logistic regression using the same preprocessing pipeline (StandardScaler + one‑hot passthrough) and C=0.03 from the tuning step.  \n",
    "This involves:  \n",
    "- Training `penalty='l1'`, `solver='saga'`, `C=0.03`  \n",
    "- Training `penalty='elasticnet'`, `solver='saga'`, `l1_ratio=0.5`, `C=0.03`  \n",
    "- Evaluating each on the test set with accuracy, confusion matrix, ROC AUC, and precision/recall\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "md-q3-l1",
   "metadata": {},
   "source": [
    "### L1‑Regularised Logistic Regression (saga)\n",
    "\n",
    "Train an L1 model with `C=0.03` to encourage sparsity in coefficients; evaluate on the held‑out test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "code-q3-l1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# L1-regularised Logistic Regression with saga\n",
    "prep_q3 = ColumnTransformer([('num', StandardScaler(), numeric_cols)], remainder='passthrough')\n",
    "pipe_l1 = Pipeline([('prep', prep_q3), ('clf', LogisticRegression(penalty='l1', solver='saga', C=0.03, max_iter=1000, tol=1e-3, n_jobs=1))])\n",
    "\n",
    "pipe_l1.fit(X_train, y_train)\n",
    "y_pred_l1 = pipe_l1.predict(X_test)\n",
    "y_proba_l1 = pipe_l1.predict_proba(X_test)[:, 1]\n",
    "\n",
    "print('L1 Accuracy:', round(accuracy_score(y_test, y_pred_l1), 4))\n",
    "print('L1 Confusion matrix:', confusion_matrix(y_test, y_pred_l1))\n",
    "print('L1 ROC AUC:', round(roc_auc_score(y_test, y_proba_l1), 4))\n",
    "print('L1 Classification report:', classification_report(y_test, y_pred_l1, digits=3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "md-q3-elastic",
   "metadata": {},
   "source": [
    "### Elastic Net Logistic Regression (saga, l1_ratio=0.5)\n",
    "\n",
    "Train an Elastic Net model combining L1 and L2 (`l1_ratio=0.5`) with `C=0.03`; evaluate on the held‑out test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "code-q3-elastic",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Elastic Net Logistic Regression with saga (l1_ratio=0.5)\n",
    "prep_q3_en = ColumnTransformer([('num', StandardScaler(), numeric_cols)], remainder='passthrough')\n",
    "pipe_en = Pipeline([('prep', prep_q3_en), ('clf', LogisticRegression(penalty='elasticnet', solver='saga', l1_ratio=0.5, C=0.03, max_iter=1000, tol=1e-3, n_jobs=1))])\n",
    "\n",
    "pipe_en.fit(X_train, y_train)\n",
    "y_pred_en = pipe_en.predict(X_test)\n",
    "y_proba_en = pipe_en.predict_proba(X_test)[:, 1]\n",
    "\n",
    "print('ElasticNet Accuracy:', round(accuracy_score(y_test, y_pred_en), 4))\n",
    "print('ElasticNet Confusion matrix:', confusion_matrix(y_test, y_pred_en))\n",
    "print('ElasticNet ROC AUC:', round(roc_auc_score(y_test, y_proba_en), 4))\n",
    "print('ElasticNet Classification report:', classification_report(y_test, y_pred_en, digits=3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "md-q3-summary",
   "metadata": {},
   "source": [
    "### Summary\n",
    "\n",
    "Two alternative regularised logistic regression models were tested:  \n",
    "- **L1 (Lasso) Logistic Regression**  \n",
    "- **Elastic Net Logistic Regression**\n",
    "\n",
    "Both models achieved performance very similar to the tuned L2 model from the tuning step: accuracy ~0.90 and ROC AUC ~0.907. Precision for the positive class was around 0.64–0.65, while recall remained low at ~0.32. \n",
    "\n",
    "This indicates that changing the regularisation type did not significantly affect performance on this dataset. However, L1 and Elastic Net regularisation are useful for feature selection and reducing model complexity. In this case, the models were stable but did not substantially improve recall for the minority class."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a048727",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "q4-writeup",
   "metadata": {},
   "source": [
    "## Model Comparison\n",
    "\n",
    "**Objective:**  \n",
    "Compare the baseline L2 logistic regression from  with the L1 and Elastic Net variants from . We report accuracy, ROC AUC, precision, and recall on the held-out test set.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "md-q4-compare",
   "metadata": {},
   "source": [
    "### Compare L2 (Baseline), L1, and Elastic Net\n",
    "\n",
    "Evaluate all three models using consistent preprocessing and metrics, then summarise results side-by-side."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "code-q4-compare",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build/evaluate three models: L2 baseline (), L1, Elastic Net ()\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, precision_score, recall_score, confusion_matrix\n",
    "\n",
    "def eval_model(name, est, X_te, y_te):\n",
    "    y_pred = est.predict(X_te)\n",
    "    y_proba = est.predict_proba(X_te)[:, 1]\n",
    "    return {\n",
    "        'Model': name,\n",
    "        'Accuracy': round(accuracy_score(y_te, y_pred), 4),\n",
    "        'ROC_AUC': round(roc_auc_score(y_te, y_proba), 4),\n",
    "        'Precision_pos': round(precision_score(y_te, y_pred, pos_label=1), 4),\n",
    "        'Recall_pos': round(recall_score(y_te, y_pred, pos_label=1), 4),\n",
    "        'ConfusionMatrix': confusion_matrix(y_te, y_pred).tolist(),\n",
    "    }\n",
    "\n",
    "# L2 baseline (recreate  pipeline to ensure availability)\n",
    "prep_l2 = ColumnTransformer([('num', StandardScaler(), numeric_cols)], remainder='passthrough')\n",
    "pipe_l2 = Pipeline([('prep', prep_l2), ('clf', LogisticRegression(solver='lbfgs', penalty='l2', max_iter=1000))])\n",
    "pipe_l2.fit(X_train, y_train)\n",
    "\n",
    "# L1 (reuse if available, else create/fit)\n",
    "try:\n",
    "    _ = pipe_l1\n",
    "except NameError:\n",
    "    prep_q3 = ColumnTransformer([('num', StandardScaler(), numeric_cols)], remainder='passthrough')\n",
    "    pipe_l1 = Pipeline([('prep', prep_q3), ('clf', LogisticRegression(penalty='l1', solver='saga', C=0.03, max_iter=1000, tol=1e-3, n_jobs=1))])\n",
    "    pipe_l1.fit(X_train, y_train)\n",
    "\n",
    "# Elastic Net (reuse if available, else create/fit)\n",
    "try:\n",
    "    _ = pipe_en\n",
    "except NameError:\n",
    "    prep_q3_en = ColumnTransformer([('num', StandardScaler(), numeric_cols)], remainder='passthrough')\n",
    "    pipe_en = Pipeline([('prep', prep_q3_en), ('clf', LogisticRegression(penalty='elasticnet', solver='saga', l1_ratio=0.5, C=0.03, max_iter=1000, tol=1e-3, n_jobs=1))])\n",
    "    pipe_en.fit(X_train, y_train)\n",
    "\n",
    "# Collect results\n",
    "import pandas as pd\n",
    "rows = []\n",
    "rows.append(eval_model('L2 (Baseline)', pipe_l2, X_test, y_test))\n",
    "rows.append(eval_model('L1 (saga, C=0.03)', pipe_l1, X_test, y_test))\n",
    "rows.append(eval_model('Elastic Net (saga, C=0.03, l1_ratio=0.5)', pipe_en, X_test, y_test))\n",
    "summary_df = pd.DataFrame(rows)[['Model','Accuracy','ROC_AUC','Precision_pos','Recall_pos']]\n",
    "summary_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "md-q4-summary",
   "metadata": {},
   "source": [
    "### Summary\n",
    "\n",
    "Across the three models, performance is very similar on the held‑out test set. Key observations:\n",
    "\n",
    "- Best ROC AUC: Elastic Net (0.9071), closely followed by L1 (0.9069) and L2 baseline (0.9054).\n",
    "- Highest accuracy and recall: L2 baseline (Accuracy 0.9016, Recall 0.3488).\n",
    "- Precision is comparable across models (~0.64–0.65), with L2 baseline slightly ahead (0.6474).\n",
    "\n",
    "Interpretation: Changing the regularisation type (L2 → L1/Elastic Net) does not materially change performance on this dataset. Elastic Net offers a marginal gain in ranking quality (ROC AUC) but does not improve positive‑class recall at the default threshold. If improving recall is a goal, consider threshold tuning (e.g., using the precision–recall curve), class_weight='balanced', or cost‑sensitive evaluation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2e5d5d8",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "q5-writeup",
   "metadata": {},
   "source": [
    "## KNN with Hyperparameter Tuning\n",
    "\n",
    "**Objective:**  \n",
    "Continue from  with the same evaluation setting by training a K‑Nearest Neighbors (KNN) classifier and tuning `n_neighbors (K)` on the training set using stratified 5‑fold CV with ROC AUC scoring.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "md-q5-pipeline",
   "metadata": {},
   "source": [
    "### Build KNN Pipeline\n",
    "\n",
    "Use the same preprocessing (StandardScaler on numeric columns via ColumnTransformer; one‑hot columns passthrough) and add a `KNeighborsClassifier` as the estimator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "code-q5-pipeline",
   "metadata": {},
   "outputs": [],
   "source": [
    "# KNN pipeline using the existing numeric scaler + one-hot passthrough\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "prep_knn = ColumnTransformer([('num', StandardScaler(), numeric_cols)], remainder='passthrough')\n",
    "pipe_knn = Pipeline([('prep', prep_knn), ('clf', KNeighborsClassifier())])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "md-q5-define",
   "metadata": {},
   "source": [
    "### Define K Grid and CV Strategy\n",
    "\n",
    "Tune `n_neighbors` over a range of odd K values to avoid ties. Use `StratifiedKFold(n_splits=5, shuffle=True, random_state=42)` and `scoring=\"roc_auc\"` (suitable for imbalanced data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "code-q5-define",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define grid and stratified CV\n",
    "from sklearn.model_selection import StratifiedKFold, GridSearchCV\n",
    "\n",
    "k_values = [3, 5, 7, 9, 11, 15, 21, 31, 41, 51]\n",
    "param_grid_knn = {'clf__n_neighbors': k_values}\n",
    "skf_knn = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "md-q5-run",
   "metadata": {},
   "source": [
    "### Run GridSearchCV (KNN)\n",
    "\n",
    "Perform stratified 5‑fold CV on the training set using ROC AUC; report the best K and mean CV ROC AUC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "code-q5-run",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grid search for KNN\n",
    "grid_knn = GridSearchCV(\n",
    "    estimator=pipe_knn,\n",
    "    param_grid=param_grid_knn,\n",
    "    scoring='roc_auc',\n",
    "    cv=skf_knn,\n",
    "    n_jobs=-1,\n",
    "    refit=True,\n",
    "    verbose=1,\n",
    "    return_train_score=True\n",
    ")\n",
    "grid_knn.fit(X_train, y_train)\n",
    "\n",
    "print('Best params (KNN):', grid_knn.best_params_)\n",
    "print('Best CV ROC AUC (KNN):', round(grid_knn.best_score_, 4))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "md-q5-plot",
   "metadata": {},
   "source": [
    "### Plot ROC AUC vs K\n",
    "\n",
    "Visualise how validation ROC AUC varies with the number of neighbours K."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "code-q5-plot",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot CV ROC AUC vs K\n",
    "import matplotlib.pyplot as plt\n",
    "cv_knn = pd.DataFrame(grid_knn.cv_results_)\n",
    "cv_knn['K'] = cv_knn['param_clf__n_neighbors'].astype(int)\n",
    "cv_knn = cv_knn.sort_values('K')\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.errorbar(cv_knn['K'], cv_knn['mean_test_score'], yerr=cv_knn['std_test_score'], fmt='-o', capsize=4)\n",
    "plt.xlabel('K (n_neighbors)')\n",
    "plt.ylabel('Mean CV ROC AUC')\n",
    "plt.title('KNN: ROC AUC vs K (5-fold Stratified CV)')\n",
    "best_k = grid_knn.best_params_['clf__n_neighbors']\n",
    "plt.axvline(best_k, color='red', linestyle='--', alpha=0.6, label=f'Best K = {best_k}')\n",
    "plt.legend()\n",
    "plt.grid(True, ls=':', alpha=0.5)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "md-q5-test-eval",
   "metadata": {},
   "source": [
    "### Evaluate Best KNN on Test\n",
    "\n",
    "Use the refit best KNN model to evaluate accuracy, confusion matrix, ROC AUC, and precision/recall on the held‑out test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "code-q5-eval",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate tuned KNN on the held-out test set\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, roc_auc_score, classification_report\n",
    "y_pred_knn = grid_knn.predict(X_test)\n",
    "y_proba_knn = grid_knn.predict_proba(X_test)[:, 1]\n",
    "\n",
    "print('KNN Test Accuracy:', round(accuracy_score(y_test, y_pred_knn), 4))\n",
    "print('KNN Test Confusion matrix:', confusion_matrix(y_test, y_pred_knn))\n",
    "print('KNN Test ROC AUC:', round(roc_auc_score(y_test, y_proba_knn), 4))\n",
    "print('KNN Test Classification report:', classification_report(y_test, y_pred_knn, digits=3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "md-q5-summary",
   "metadata": {},
   "source": [
    "### Summary\n",
    "\n",
    "The optimal K for KNN was **51**, yielding a mean CV ROC AUC of 0.9078 and test ROC AUC of 0.904. On the test set, accuracy was 0.896 with precision of 0.67 and recall of 0.22 for the positive class.\n",
    "\n",
    "Compared to logistic regression models (baseline, tuned, L1, and Elastic Net), KNN showed similar ROC AUC but lower recall. While precision for the minority class was slightly higher, KNN missed a larger proportion of positive cases.\n",
    "\n",
    "Logistic regression models are more efficient, requiring only ~42 trainable parameters, whereas KNN stores all training samples and incurs high prediction-time costs. Logistic regression also benefits from regularisation, which improves generalisation and stability.\n",
    "\n",
    "KNN underperforms mainly due to the **curse of dimensionality** in the 42-feature space and the imbalanced nature of the dataset. Logistic regression provided better balance between precision and recall, making it more suitable for this problem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e08f1581",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b281bf76",
   "metadata": {
    "id": "viz-utils"
   },
   "source": [
    "## Visualisation of Results\n",
    "\n",
    "We visualise the selected model’s performance to support the discussion and conclusions:\n",
    "- **Confusion matrix**: shows error types and class balance at a chosen threshold.\n",
    "- **ROC curve**: ranking quality across thresholds (AUC as a summary).\n",
    "- **Precision–Recall**: positive‑class performance under imbalance (AP as a summary).\n",
    "\n",
    "Review these plots before the final conclusions to ground the takeaways in the observed trade‑offs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69a60ec0",
   "metadata": {
    "id": "viz-final"
   },
   "outputs": [],
   "source": [
    "# Visualisation helpers (confusion matrix, ROC, PR curves)\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import (roc_curve, auc, precision_recall_curve,\n",
    "                             average_precision_score, ConfusionMatrixDisplay)\n",
    "\n",
    "def pick_estimator():\n",
    "    # Prefer the tuned model if available, else fall back to a baseline pipeline\n",
    "    g = globals()\n",
    "    for name in ['grid', 'grid_search', 'best_model']:\n",
    "        if name in g and hasattr(g[name], 'best_estimator_'):\n",
    "            return g[name].best_estimator_\n",
    "    for name in ['pipe', 'pipe_l2', 'clf', 'model']:\n",
    "        if name in g:\n",
    "            return g[name]\n",
    "    raise NameError('No fitted estimator found. Fit a model first.')\n",
    "\n",
    "def ensure_preds(est, X_te, y_te):\n",
    "    y_pred = est.predict(X_te)\n",
    "    if hasattr(est, 'predict_proba'):\n",
    "        y_proba = est.predict_proba(X_te)[:, 1]\n",
    "    else:\n",
    "        # For models without predict_proba (e.g., SVM without probability), use decision_function\n",
    "        y_proba = getattr(est, 'decision_function')(X_te)\n",
    "        # Min‑max scale decision scores to [0,1] for curves\n",
    "        y_min, y_max = y_proba.min(), y_proba.max()\n",
    "        if y_max > y_min:\n",
    "            y_proba = (y_proba - y_min) / (y_max - y_min)\n",
    "    return y_pred, y_proba\n",
    "\n",
    "def plot_confusion(y_true, y_pred, title='Confusion Matrix'):\n",
    "    fig, ax = plt.subplots(figsize=(5,4))\n",
    "    ConfusionMatrixDisplay.from_predictions(y_true, y_pred, cmap='Blues', ax=ax)\n",
    "    ax.set_title(title)\n",
    "    plt.tight_layout()\n",
    "\n",
    "def plot_roc(y_true, y_score, title='ROC Curve'):\n",
    "    fpr, tpr, _ = roc_curve(y_true, y_score)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    fig, ax = plt.subplots(figsize=(5,4))\n",
    "    ax.plot(fpr, tpr, label=f'AUC = {roc_auc:.3f}')\n",
    "    ax.plot([0,1], [0,1], 'k--', alpha=0.6)\n",
    "    ax.set_xlabel('False Positive Rate')\n",
    "    ax.set_ylabel('True Positive Rate')\n",
    "    ax.set_title(title)\n",
    "    ax.legend(loc='lower right')\n",
    "    plt.tight_layout()\n",
    "\n",
    "def plot_pr(y_true, y_score, title='Precision–Recall Curve'):\n",
    "    precision, recall, _ = precision_recall_curve(y_true, y_score)\n",
    "    ap = average_precision_score(y_true, y_score)\n",
    "    fig, ax = plt.subplots(figsize=(5,4))\n",
    "    ax.plot(recall, precision, label=f'AP = {ap:.3f}')\n",
    "    ax.set_xlabel('Recall')\n",
    "    ax.set_ylabel('Precision')\n",
    "    ax.set_title(title)\n",
    "    ax.legend(loc='lower left')\n",
    "    plt.tight_layout()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69a60ec0",
   "metadata": {
    "id": "viz-final"
   },
   "outputs": [],
   "source": [
    "# Generate professional plots using the chosen estimator\n",
    "est = pick_estimator()\n",
    "y_pred, y_score = ensure_preds(est, X_test, y_test)\n",
    "\n",
    "plot_confusion(y_test, y_pred, title='Confusion Matrix — Selected Model')\n",
    "plot_roc(y_test, y_score, title='ROC Curve — Selected Model')\n",
    "plot_pr(y_test, y_score, title='Precision–Recall — Selected Model')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17acebbb",
   "metadata": {
    "id": "conclusion-next"
   },
   "source": [
    "## Conclusion & Next Steps\n",
    "\n",
    "The models achieve strong overall ranking performance (ROC AUC ≈ 0.90), with high precision but lower recall for the positive class, reflecting the dataset's imbalance. For a marketing use case, this suggests careful threshold selection and cost‑sensitive evaluation are needed to balance conversion lift against contact costs.\n",
    "\n",
    "Key insights:\n",
    "\n",
    "- **Logistic regression** provided stable performance (~0.90 accuracy, ~0.907 ROC AUC). Regularisation (L1, L2, Elastic Net) balanced complexity and generalisation.\n",
    "- **KNN** achieved similar ROC AUC but required many neighbours (K=51) and had lower recall — inefficient in high‑dimensional, imbalanced data.\n",
    "- Looking beyond accuracy (ROC AUC, precision, recall) was essential to assess models fairly under class imbalance.\n",
    "\n",
    "Overall, this project demonstrated a complete supervised‑learning workflow: preprocessing, baseline modelling, hyperparameter tuning, regularisation, and algorithm comparison, highlighting trade‑offs between simplicity, interpretability, and predictive performance.\n",
    "\n",
    "Next steps:\n",
    "- Threshold tuning with cost assumptions (e.g., maximise expected net benefit).\n",
    "- Probability calibration and lift/gain analysis to support campaign planning.\n",
    "- Compare against tree‑based models (Random Forest, Gradient Boosting) and interpret with SHAP.\n",
    "- Address imbalance via class weights or resampling (SMOTE) and re‑evaluate PR metrics.\n",
    "- Cross‑validation on temporal splits if campaign timing matters.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}